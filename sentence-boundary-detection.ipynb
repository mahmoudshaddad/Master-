{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence-boundary-detection\n",
    "\\There are several libraries and tools that provide sentence boundary detection capabilities in natural language processing (NLP). Here are some of the popular libraries and tools:\n",
    "\n",
    "spaCy:\n",
    "\n",
    "Website: https://spacy.io/\n",
    "spaCy is a popular NLP library that includes a pretrained model for sentence boundary detection. You can use it to tokenize and segment text into sentences.\n",
    "NLTK (Natural Language Toolkit):\n",
    "\n",
    "Website: https://www.nltk.org/\n",
    "NLTK is a comprehensive NLP library for Python. It provides tools for tokenization and sentence splitting, making it easy to perform sentence boundary detection.\n",
    "CoreNLP:\n",
    "\n",
    "Website: https://stanfordnlp.github.io/CoreNLP/\n",
    "Stanford CoreNLP is a suite of natural language processing tools developed by Stanford University. It includes a sentence splitter module that can identify sentence boundaries.\n",
    "OpenNLP:\n",
    "\n",
    "Website: https://opennlp.apache.org/\n",
    "Apache OpenNLP is an open-source NLP library that provides various NLP tools, including sentence detection models for multiple languages.\n",
    "Gensim:\n",
    "\n",
    "Website: https://radimrehurek.com/gensim/\n",
    "Gensim is a library for topic modeling and document similarity analysis, but it also includes tools for sentence segmentation.\n",
    "TextBlob:\n",
    "\n",
    "Website: https://textblob.readthedocs.io/\n",
    "TextBlob is a simple NLP library for Python. It includes a sentence tokenizer to segment text into sentences.\n",
    "Stanza (formerly known as StanfordNLP):\n",
    "\n",
    "Website: https://stanfordnlp.github.io/stanza/\n",
    "Stanza is an NLP library that provides pretrained models for various NLP tasks, including sentence splitting.\n",
    "Apache Tika:\n",
    "\n",
    "Website: https://tika.apache.org/\n",
    "Apache Tika is a content analysis toolkit that can be used for extracting text from various document formats. It includes a sentence boundary detector.\n",
    "CLTK (Classical Language Toolkit):\n",
    "\n",
    "Website: https://cltk.org/\n",
    "CLTK is focused on classical languages and includes tools for sentence segmentation in ancient texts.\n",
    "SentencePiece:\n",
    "\n",
    "Website: https://github.com/google/sentencepiece\n",
    "SentencePiece is a library developed by Google for text tokenization, including sentence segmentation. It's commonly used in natural language processing tasks for various languages.\n",
    "These libraries offer a range of options for sentence boundary detection, with some of them providing pretrained models for multiple languages. You can choose the one that best fits your specific NLP project's needs and programming language preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy's built-in sentence boundary detection model (sentencizer) is a good choice for many languages, including German. However, if you're specifically looking for the best models for sentence boundary detection in the German language, you can consider the following options:\n",
    "\n",
    "SpaCy's Pretrained Models for German:\n",
    "\n",
    "SpaCy provides pretrained models for various languages, including German. You can use these models for sentence boundary detection. Some popular SpaCy models for German are de_core_news_sm, de_core_news_md, and de_core_news_lg.\n",
    "BERT-Based Models:\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) models are powerful for sentence boundary detection and many other NLP tasks. You can use multilingual BERT models, like mBERT (multilingual BERT), which can handle multiple languages, including German.\n",
    "GPT-Based Models:\n",
    "\n",
    "GPT (Generative Pretrained Transformer) models can be fine-tuned for specific tasks, including sentence boundary detection. You can use a German GPT-3 model or fine-tune a general GPT-3 model on German data for this purpose.\n",
    "Customized Rule-Based Models:\n",
    "\n",
    "Depending on your specific needs, you can create custom rule-based models for German sentence boundary detection. These rules can be based on language-specific punctuation and grammatical patterns.\n",
    "Language-Specific NLP Libraries:\n",
    "\n",
    "You can explore other NLP libraries or tools that specialize in German language processing. Libraries like Stanza (formerly known as StanfordNLP) provide models and tools for German NLP tasks, including sentence boundary detection.\n",
    "The choice of the best model depends on your specific use case and the trade-off between model accuracy and computational resources. If you're already using SpaCy for other NLP tasks in German, it might be convenient to stick with SpaCy's built-in models. However, if you require the highest accuracy or need to fine-tune a model for a specific domain, you might consider more advanced models like BERT or GPT-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "\n",
    "warnings.simplefilter(\"ignore\", InconsistentVersionWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "fname = r'C:\\A\\00Master\\Demo\\data acc\\DIN EN 206.pdf'\n",
    "doc = fitz.open(fname)\n",
    "rawtext = ''\n",
    "for page in doc:\n",
    "    rawtext += page.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clearing text from noise before tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "# Sentences to delete\n",
    "sentences_to_delete = [\n",
    "    \"Technische Universität Darmstadt\",\n",
    "    \"Printed copies are uncontrolled\",\n",
    "    \"— Leerseite —\",\n",
    "\n",
    "]\n",
    "\n",
    "# Create a regular expression pattern to match the sentences\n",
    "pattern = \"|\".join(map(re.escape, sentences_to_delete))\n",
    "\n",
    "# Use re.sub to remove the matched sentences\n",
    "new_text = re.sub(pattern, \"\", rawtext)\n",
    "\n",
    "# print(new_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the Text without the cleaning : \" + str(len(rawtext)) + \"\\n\" +\n",
    "      \"The length of the Text with the cleaning of Tu Darmstadt & white space: \" + str(len(new_text)) + \"\\n\" +\n",
    "      \"The difference between the Text is: \" + str(len(rawtext) - len(new_text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text2 = re.sub(r'\\s+', ' ', new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the Text without the cleaning : \" + str(len(new_text)) + \"\\n\" +\n",
    "      \"The length of the Text with the cleaning of Tu Darmstadt & white space: \" + str(len(new_text2)) + \"\\n\" +\n",
    "      \"The difference between the Text is: \" + str(len(new_text) - len(new_text2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_unknown_characters(text, replacement=\" \"):\n",
    "    return text.replace('�', replacement)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "new_text3 = replace_unknown_characters(new_text2)\n",
    "print(new_text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the Text without the cleaning : \" + str(len(new_text2)) + \"\\n\" +\n",
    "      \"The length of the Text with the cleaning of Tu Darmstadt & white space: \" + str(len(new_text3)) + \"\\n\" +\n",
    "      \"The difference between the Text is: \" + str(len(new_text2) - len(new_text3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regular expression pattern to match the date and time format\n",
    "pattern = r'\\d{2}/\\d{2}/\\d{4} \\d{2}:\\d{2}:\\d{2}'\n",
    "\n",
    "# Use re.sub to replace all occurrences of the pattern with an empty string\n",
    "new_text4 = re.sub(pattern, '', new_text3)\n",
    "\n",
    "# Print the cleaned text\n",
    "print(new_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the Text without the cleaning : \" + str(len(new_text3)) + \"\\n\" +\n",
    "      \"The length of the Text with the cleaning of Tu Darmstadt & white space: \" + str(len(new_text4)) + \"\\n\" +\n",
    "      \"The difference between the Text is: \" + str(len(new_text3) - len(new_text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of the Text without the cleaning : \" + str(len(rawtext)) + \"\\n\" +\n",
    "      \"The length of the Text with the cleaning of Tu Darmstadt & white space: \" + str(len(new_text4)) + \"\\n\" +\n",
    "      \"The totall clean character after deleting the noise is : \" + str(len(rawtext) - len(new_text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text= new_text4\n",
    "print(\"The length of the Text: \"+ str(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <center><b>Sentence Boundary Detection </b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   de_dep_news_trf: A large Transformer-based model suitable for various NLP tasks, resource-intensive. for one pdf take aroud 1m 35.6s for DIN 206\n",
    "-   de_core_news_lg: A large, versatile model for general-purpose NLP with good performance.\n",
    "-   de_core_news_md: A medium-sized model, balanced for various NLP tasks with moderate resource requirements.\n",
    "-   de_core_news_sm: A small, lightweight model designed for basic NLP tasks with minimal resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## without deleting the Puntutions and apply thte Sentence Tokenzation -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    txt = \"\".join([x for x in text if x not in string.punctuation])\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\A\\00Master\\Demo\\data acc\\sentence-boundary-detection.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sentences\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Remove punctuation and tokenize using spaCy\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m text_SpaCy \u001b[39m=\u001b[39m tokenize(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Now you can access individual sentences without punctuation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(text_SpaCy[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;32mc:\\A\\00Master\\Demo\\data acc\\sentence-boundary-detection.ipynb Cell 25\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(text):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     sentences \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/A/00Master/Demo/data%20acc/sentence-boundary-detection.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m sentences\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy\\language.py:1049\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1048\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1049\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy_curated_transformers\\pipeline\\transformer.py:240\u001b[0m, in \u001b[0;36mCuratedTransformer.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39m# To ensure that the model's internal state is always consistent with the pipe's.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_model_all_layer_outputs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_layer_outputs)\n\u001b[1;32m--> 240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(docs)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    331\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m, X, is_train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy_curated_transformers\\models\\architectures.py:649\u001b[0m, in \u001b[0;36mtransformer_model_forward\u001b[1;34m(model, docs, is_train)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransformer_model_forward\u001b[39m(\n\u001b[0;32m    647\u001b[0m     model: TransformerModelT, docs: TransformerInT, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m    648\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[TransformerOutT, TransformerBackpropT]:\n\u001b[1;32m--> 649\u001b[0m     Y, backprop_layer \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](docs, is_train\u001b[39m=\u001b[39mis_train)\n\u001b[0;32m    651\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY):\n\u001b[0;32m    652\u001b[0m         backprop_layer(dY)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m, X, is_train\u001b[39m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy_curated_transformers\\models\\with_non_ws_tokens.py:72\u001b[0m, in \u001b[0;36mwith_non_ws_tokens_forward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     70\u001b[0m inner: Model[Tok2PiecesInT, WsTokenAdapterOutT] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[0;32m     71\u001b[0m tokens, ws_counts \u001b[39m=\u001b[39m _filter_tokens(X)\n\u001b[1;32m---> 72\u001b[0m Y_no_ws, backprop_no_ws \u001b[39m=\u001b[39m inner(tokens, is_train)\n\u001b[0;32m     74\u001b[0m \u001b[39m# Note: we modify the model outputs in-place. Since we are wrapping the\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m# model, there should be no other consumers. Not sure yet if the same\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39m# applies to the gradient (e.g. consider summing two encoders of the\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m# same width downstream.)\u001b[39;00m\n\u001b[0;32m     79\u001b[0m alignments \u001b[39m=\u001b[39m _create_alignments(model, Y_no_ws, ws_counts)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m, X, is_train\u001b[39m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39mis_train)\n\u001b[0;32m     55\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     56\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m, X, is_train\u001b[39m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\spacy_curated_transformers\\models\\with_strided_spans.py:108\u001b[0m, in \u001b[0;36mwith_strided_spans_forward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m    106\u001b[0m outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m _split_spans(spans, batch_size):\n\u001b[1;32m--> 108\u001b[0m     output, bp \u001b[39m=\u001b[39m transformer(cast(TorchTransformerInT, batch), is_train\u001b[39m=\u001b[39mis_train)\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(output, TransformerModelOutput):\n\u001b[0;32m    110\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    111\u001b[0m             Errors\u001b[39m.\u001b[39mE014\u001b[39m.\u001b[39mformat(model_name\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mname, input_type\u001b[39m=\u001b[39m\u001b[39mtype\u001b[39m(output))\n\u001b[0;32m    112\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    308\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    309\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func(\u001b[39mself\u001b[39m, X, is_train\u001b[39m=\u001b[39mis_train)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\layers\\pytorchwrapper.py:225\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m    222\u001b[0m convert_outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mattrs[\u001b[39m\"\u001b[39m\u001b[39mconvert_outputs\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    224\u001b[0m Xtorch, get_dX \u001b[39m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[1;32m--> 225\u001b[0m Ytorch, torch_backprop \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mshims[\u001b[39m0\u001b[39m](Xtorch, is_train)\n\u001b[0;32m    226\u001b[0m Y, get_dYtorch \u001b[39m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[0;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dY: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\shims\\pytorch.py:97\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[1;34m(self, inputs, is_train)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbegin_update(inputs)\n\u001b[0;32m     96\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(inputs), \u001b[39mlambda\u001b[39;00m a: \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\thinc\\shims\\pytorch.py:115\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mixed_precision):\n\u001b[1;32m--> 115\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model(\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    117\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\curated_transformers\\models\\curated_transformer.py:37\u001b[0m, in \u001b[0;36mCuratedTransformer.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     28\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     29\u001b[0m     input_ids: Tensor,\n\u001b[0;32m     30\u001b[0m     attention_mask: Optional[AttentionMask] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     31\u001b[0m     token_type_ids: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m PyTorchTransformerOutput:\n\u001b[0;32m     33\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m    Shapes:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m        input_ids, attention_mask, token_type_ids - (batch, seq_len)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurated_encoder\u001b[39m.\u001b[39mforward(input_ids, attention_mask, token_type_ids)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\curated_transformers\\models\\bert\\encoder.py:52\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     50\u001b[0m layer_outputs \u001b[39m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 52\u001b[0m     layer_output \u001b[39m=\u001b[39m layer(layer_output, attention_mask)\n\u001b[0;32m     53\u001b[0m     layer_outputs\u001b[39m.\u001b[39mappend(layer_output)\n\u001b[0;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m PyTorchTransformerOutput(\n\u001b[0;32m     56\u001b[0m     embedding_output\u001b[39m=\u001b[39membeddings, layer_hidden_states\u001b[39m=\u001b[39mlayer_outputs\n\u001b[0;32m     57\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\curated_transformers\\models\\bert\\layer.py:133\u001b[0m, in \u001b[0;36mBertEncoderLayer.forward\u001b[1;34m(self, x, attn_mask)\u001b[0m\n\u001b[0;32m    130\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_output_dropout(attn_out)\n\u001b[0;32m    131\u001b[0m attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattn_output_layernorm(x \u001b[39m+\u001b[39m attn_out)\n\u001b[1;32m--> 133\u001b[0m ffn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn(attn_out)\n\u001b[0;32m    134\u001b[0m ffn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn_output_dropout(ffn_out)\n\u001b[0;32m    135\u001b[0m ffn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffn_output_layernorm(attn_out \u001b[39m+\u001b[39m ffn_out)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\curated_transformers\\models\\bert\\layer.py:100\u001b[0m, in \u001b[0;36mBertFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m     96\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[39m    Shapes:\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m        x - (batch, seq_len, width)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(x)\n\u001b[0;32m    101\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(out)\n\u001b[0;32m    102\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(out)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engin\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy language model\n",
    "nlp = spacy.load(\"de_dep_news_trf\")\n",
    "\n",
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    return sentences\n",
    "   \n",
    "\n",
    "# Remove punctuation and tokenize using spaCy\n",
    "text_SpaCy = tokenize(text)\n",
    "\n",
    "# Now you can access individual sentences without punctuation\n",
    "print(text_SpaCy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text_SpaCy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'Index': range(1, len(text_SpaCy) + 1), 'Sentence': text_SpaCy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noPun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noPun.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StopWords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd \n",
    "\n",
    "# Load the spaCy language model for German\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Get the list of stop words in German\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "# Print the list of stop words\n",
    "print(\"List of stop words in German:\")\n",
    "print(stop_words)\n",
    "df_stopword = pd.DataFrame(stop_words, columns=['stop_words_SpaCy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "stop_words = set(stopwords.words(\"german\"))\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE)\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\")\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE)\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE)\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text)\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = clean_text(text, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_text)\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download the Punkt tokenizer for German if you haven't already\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the text into sentences\n",
    "sentences_NLTK = sent_tokenize(text, language=\"german\")\n",
    "\n",
    "# Print the detected sentences\n",
    "for sentence_NLTK in sentences_NLTK:\n",
    "    print(sentence_NLTK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_NLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get the list of sentences\n",
    "sentences_Blob = blob.sentences\n",
    "\n",
    "# Print the detected sentences\n",
    "for sentence in sentences_Blob:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences_Blob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wtpsplit\n",
    "https://github.com/bminixhofer/wtpsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wtpsplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wtpsplit import WtP\n",
    "wtp = WtP(\"wtp-bert-mini\")\n",
    "sentence_wtp=wtp.split(text, lang_code=\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence_wtp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
